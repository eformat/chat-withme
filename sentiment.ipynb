{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Sentiment Analysis\n",
    "\n",
    "`https://spacy.io` is a library that has methods and pre-trained models for parsing natural language and determining parts of speech, etc., in many languages\n",
    "\n",
    "`VADER` is a library that can characterize the sentiments of individual sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example simplified usage\n",
    "\n",
    "Import spaCy and the english model trained on web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the analyzer class from VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some simplified chat text from our chat app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"\"\"User jane left\n",
    ">> jane: merci\n",
    ">> support3: sure .. coming right up\n",
    ">> jane: black coffee, no milk, no sugar please !\n",
    ">> support3: hi, how can we help ?\n",
    ">> jane: hi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell spaCy to use english to parse the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = english(sampletext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what we can do with a `spacy.tokens.doc.Doc` by using `help(result)` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we can do is use spaCy to indentify speech in natural language text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User PROPN\n",
      "jane NOUN\n",
      "left VERB\n",
      "\n",
      " SPACE\n",
      "> X\n",
      "> PUNCT\n",
      "jane NOUN\n",
      ": PUNCT\n",
      "merci PROPN\n",
      "\n",
      " SPACE\n",
      "> X\n",
      "> X\n",
      "support3 NOUN\n",
      ": PUNCT\n",
      "sure ADJ\n",
      ".. PUNCT\n",
      "coming VERB\n",
      "right ADV\n",
      "up ADV\n",
      "\n",
      " SPACE\n",
      "> X\n",
      "> PUNCT\n",
      "jane NOUN\n",
      ": PUNCT\n",
      "black ADJ\n",
      "coffee NOUN\n",
      ", PUNCT\n",
      "no DET\n",
      "milk NOUN\n",
      ", PUNCT\n",
      "no DET\n",
      "sugar NOUN\n",
      "please INTJ\n",
      "! PUNCT\n",
      "\n",
      " SPACE\n",
      "> X\n",
      "> X\n",
      "support3 NOUN\n",
      ": PUNCT\n",
      "hi INTJ\n",
      ", PUNCT\n",
      "how SCONJ\n",
      "can AUX\n",
      "we PRON\n",
      "help VERB\n",
      "? PUNCT\n",
      "\n",
      " SPACE\n",
      "> X\n",
      "> PUNCT\n",
      "jane NOUN\n",
      ": PUNCT\n",
      "hi NUM\n",
      "\n",
      " SPACE\n"
     ]
    }
   ],
   "source": [
    "for token in result:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the VADER analyzer now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the sentiment scores for each sentence: negative (neg), neutral (neu), positive (pos), and overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.184, 'neu': 0.618, 'pos': 0.198, 'compound': 0.0818},\n",
       " {'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.4019},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[analyzer.polarity_scores(str(s)) for s in list(result.sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming NLP\n",
    "\n",
    "We want to do this analysis with Spark so we can stream the data. Lets run this locally for now, adjust the parameters to suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.9\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3.9\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-11.0.13.0.8-1.fc34.x86_64\"\n",
    "\n",
    "spark_master = \"local[*]\"\n",
    "app_name = \"sentiment-analysis\"\n",
    "driver_memory = '8g'\n",
    "executor_memory = '8g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 15:17:06 WARN Utils: Your hostname, virt resolves to a loopback address: 127.0.0.1; using 192.168.86.109 instead (on interface wlp2s0)\n",
      "21/11/22 15:17:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mike/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/mike/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mike/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mike/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5fbfd9a0-6e45-4509-8a11-a583a0207707;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in local-m2-cache\n",
      ":: resolution report :: resolve 540ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from local-m2-cache in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5fbfd9a0-6e45-4509-8a11-a583a0207707\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/11ms)\n",
      "21/11/22 15:17:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.86.109:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sentiment-analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4326449ca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master(spark_master) \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.eventLog.enabled\", True) \\\n",
    "    .config(\"spark.eventLog.dir\", \".\") \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", 1) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserialize the JSON message payloads from Kafka and process using spark data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "structure = StructType([StructField(fn, StringType(), True) for fn in \"id username supportname message timestamp\".split()])\n",
    "\n",
    "records = session \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"chats\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load() \\\n",
    "  .select(col(\"key\").cast(\"string\"),from_json(col(\"value\").cast(\"string\"), structure).alias(\"json\")) \\\n",
    "  .select(col(\"json.id\").alias(\"id\"), \\\n",
    "          col(\"json.username\").alias(\"username\"), \\\n",
    "          col(\"json.supportname\").alias(\"supportname\"), \\\n",
    "          col(\"json.message\").alias(\"message\"), \\\n",
    "          col(\"json.timestamp\").alias(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at top 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='karen-support', username='karen', supportname='support', message='>> karen: hi', timestamp='2021-11-22T03:29:56.956+00:00'),\n",
       " Row(id='karen-support', username='karen', supportname='support', message='>> support: hi karen, how can i help today?', timestamp='2021-11-22T03:30:05.540+00:00'),\n",
       " Row(id='karen-support', username='karen', supportname='support', message='>> karen: i hate this product, it sucks !', timestamp='2021-11-22T03:30:15.506+00:00'),\n",
       " Row(id='karen-support', username='karen', supportname='support', message='>> support: what seems to be the problem ?', timestamp='2021-11-22T03:30:25.205+00:00'),\n",
       " Row(id='karen-support', username='karen', supportname='support', message='>> karen: i cant even get my app deployed !', timestamp='2021-11-22T03:30:33.278+00:00')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically with Spark programs, we'd prefer to broadcast large data like models, but the spaCy model is tricky to serialize. So instead, we'll use this trick suggested by the Sparkling Pandas library, essentially simulating lazily-initialized worker-local storage for Spacy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is borrowed from Sparkling Pandas; see here:\n",
    "# https://github.com/sparklingpandas/sparklingml/blob/627c8f23688397a53e2e9e805e92a54c2be1cf3d/sparklingml/transformation_functions.py#L53\n",
    "class SpacyMagic(object):\n",
    "    \"\"\"\n",
    "    Simple Spacy Magic to minimize loading time.\n",
    "    >>> SpacyMagic.get(\"en\")\n",
    "    <spacy.en.English ...\n",
    "    \"\"\"\n",
    "    _spacys = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, lang):\n",
    "        if lang not in cls._spacys:\n",
    "            import spacy\n",
    "            cls._spacys[lang] = spacy.load(lang)\n",
    "        return cls._spacys[lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user-defined function to split chat messages into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def split_sentences_impl(s):\n",
    "    \"\"\" splits an English string into sentences, using spaCy \"\"\"\n",
    "    english = SpacyMagic.get(\"en_core_web_sm\")\n",
    "    return [str(sentence) for sentence in english(s).sents]\n",
    "\n",
    "split_sentences = udf(split_sentences_impl, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this looks like, we'll run it on the first 10 rows of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[Row(timestamp='2021-11-22T03:29:56.956+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> karen: hi']),\n",
       " Row(timestamp='2021-11-22T03:30:05.540+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> support: hi karen, how can i help today?']),\n",
       " Row(timestamp='2021-11-22T03:30:15.506+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> karen: i hate this product, it sucks !']),\n",
       " Row(timestamp='2021-11-22T03:30:25.205+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> support: what seems to be the problem ?']),\n",
       " Row(timestamp='2021-11-22T03:30:33.278+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> karen: i cant even get my app deployed !']),\n",
       " Row(timestamp='2021-11-22T03:30:47.308+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> support: that does not sound great.', 'lets start from the beggining']),\n",
       " Row(timestamp='2021-11-22T03:30:49.954+00:00', id='karen-support', username='karen', supportname='support', sentences=['>> karen: ok']),\n",
       " Row(timestamp='2021-11-22T03:54:49.924+00:00', id='karen-support', username='karen', supportname='support', sentences=['User karen left'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_records = records \\\n",
    "  .orderBy(\"timestamp\") \\\n",
    "  .limit(10) \\\n",
    "  .select(\"timestamp\", \"id\", \"username\", \"supportname\", split_sentences(col(\"message\")).alias(\"sentences\")) \\\n",
    "  .cache()\n",
    "\n",
    "split_records.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explode each array into multiple rows to make further processing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------+--------+-----------+-------------------------------------------+\n",
      "|timestamp                    |id           |username|supportname|sentence                                   |\n",
      "+-----------------------------+-------------+--------+-----------+-------------------------------------------+\n",
      "|2021-11-22T03:29:56.956+00:00|karen-support|karen   |support    |>> karen: hi                               |\n",
      "|2021-11-22T03:30:05.540+00:00|karen-support|karen   |support    |>> support: hi karen, how can i help today?|\n",
      "|2021-11-22T03:30:15.506+00:00|karen-support|karen   |support    |>> karen: i hate this product, it sucks !  |\n",
      "|2021-11-22T03:30:25.205+00:00|karen-support|karen   |support    |>> support: what seems to be the problem ? |\n",
      "|2021-11-22T03:30:33.278+00:00|karen-support|karen   |support    |>> karen: i cant even get my app deployed !|\n",
      "|2021-11-22T03:30:47.308+00:00|karen-support|karen   |support    |>> support: that does not sound great.     |\n",
      "|2021-11-22T03:30:47.308+00:00|karen-support|karen   |support    |lets start from the beggining              |\n",
      "|2021-11-22T03:30:49.954+00:00|karen-support|karen   |support    |>> karen: ok                               |\n",
      "|2021-11-22T03:54:49.924+00:00|karen-support|karen   |support    |User karen left                            |\n",
      "+-----------------------------+-------------+--------+-----------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "sentences = split_records.select(\"timestamp\", \"id\", \"username\", \"supportname\", explode(col(\"sentences\")).alias(\"sentence\"))\n",
    "sentences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our user-defined function for VADER scoring: it will take text and return a sentiment structure. Note that we are actually creating a broadcast variable for the VADER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "sentiment_fields = \"pos neg neu compound\".split()\n",
    "sentiment_structure = StructType([StructField(fn, FloatType(), True) for fn in sentiment_fields])\n",
    "\n",
    "analyzer_bcast = session.sparkContext.broadcast(analyzer)\n",
    "\n",
    "def vader_impl(s):\n",
    "    va = analyzer_bcast.value\n",
    "    result = va.polarity_scores(s)\n",
    "    return [result[key] for key in sentiment_fields]\n",
    "\n",
    "sentiment_score = udf(vader_impl, sentiment_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate each sentence with its sentiment and order from most negative to most positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------+--------+-----------+-------------------------------------------+----------------------------+\n",
      "|timestamp                    |id           |username|supportname|sentence                                   |sentiment                   |\n",
      "+-----------------------------+-------------+--------+-----------+-------------------------------------------+----------------------------+\n",
      "|2021-11-22T03:30:15.506+00:00|karen-support|karen   |support    |>> karen: i hate this product, it sucks !  |{0.0, 0.481, 0.519, -0.7574}|\n",
      "|2021-11-22T03:30:47.308+00:00|karen-support|karen   |support    |>> support: that does not sound great.     |{0.246, 0.3, 0.455, -0.1516}|\n",
      "|2021-11-22T03:30:47.308+00:00|karen-support|karen   |support    |lets start from the beggining              |{0.0, 0.0, 1.0, 0.0}        |\n",
      "|2021-11-22T03:29:56.956+00:00|karen-support|karen   |support    |>> karen: hi                               |{0.0, 0.0, 1.0, 0.0}        |\n",
      "|2021-11-22T03:30:33.278+00:00|karen-support|karen   |support    |>> karen: i cant even get my app deployed !|{0.0, 0.0, 1.0, 0.0}        |\n",
      "|2021-11-22T03:54:49.924+00:00|karen-support|karen   |support    |User karen left                            |{0.0, 0.0, 1.0, 0.0}        |\n",
      "|2021-11-22T03:30:25.205+00:00|karen-support|karen   |support    |>> support: what seems to be the problem ? |{0.218, 0.218, 0.565, 0.0}  |\n",
      "|2021-11-22T03:30:49.954+00:00|karen-support|karen   |support    |>> karen: ok                               |{0.524, 0.0, 0.476, 0.296}  |\n",
      "|2021-11-22T03:30:05.540+00:00|karen-support|karen   |support    |>> support: hi karen, how can i help today?|{0.435, 0.0, 0.565, 0.6597} |\n",
      "+-----------------------------+-------------+--------+-----------+-------------------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences \\\n",
    "  .select(\"timestamp\", \"id\", \"username\", \"supportname\", \"sentence\", sentiment_score(col(\"sentence\")).alias(\"sentiment\")) \\\n",
    "  .orderBy(\"sentiment.compound\") \\\n",
    "  .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
